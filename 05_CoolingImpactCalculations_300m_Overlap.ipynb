{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "#from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parks_gdf with columns: Index(['osm_id', 'OBJECTID', 'FEATURE_ANAME', 'MUNICIPALITY', 'DISTRICT',\n",
      "       'WALKING_TRACK', 'GREEN_AREAS', 'LAYERID', 'LAYERANAME', 'Validation',\n",
      "       'Park_id', 'area_m2', 'perimeter_m', 'LSI', 'ndvi_mean',\n",
      "       'ndvi_pixel_count', 'pisi_mean', 'pisi_pixel_count', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Load pre-existing park data\n",
    "parks_gdf = gpd.read_file('Data/Riyadh_parks_stats_2024.geojson')\n",
    "print(\"Loaded parks_gdf with columns:\", parks_gdf.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File link: https://drive.google.com/file/d/1OCiJFoE-HmlpSLjt37WhoBhgVWdKSz0W/view?usp=sharing\n",
    "# Then it should be added to a folder called Raster within the Data folder\n",
    "\n",
    "# Define LST file path\n",
    "lst_2024_path = 'Data/Raster/Riyadh_LST_EPSG20438_2024.tif' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open LST raster to access pixel values and transformation parameters.\n",
    "with rasterio.open(lst_2024_path) as lst_src:\n",
    "    lst_transform = lst_src.transform\n",
    "    lst_crs = lst_src.crs\n",
    "    lst_data = lst_src.read(1)\n",
    "   # print(np.nanmin(lst_data), np.nanmax(lst_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Buffer Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each park, createt buffers at 30m intervals up to 300m for the comparison of LST across different zones, providing insights into the urban heat island effect and park cooling influence. ([Cai et al., 2023](https://www.researchgate.net/publication/374556563_Cooling_island_effect_in_urban_parks_from_the_perspective_of_internal_park_landscape); [Zhang et al., 2024](https://www.nature.com/articles/s41598-024-67277-2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate Buffers (30m intervals up to 300m)\n",
    "buffer_distances = range(30, 301, 30)  # [30, 60, 90,..., 300]\n",
    "\n",
    "# Initialize buffer_gdf with a sample row to set dtypes, ensuring Park_id as int\n",
    "# and to avoid warning i got:  The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\n",
    "initial_row = {'Park_id': 0, 'distance': 0, 'geometry': Polygon()}\n",
    "buffer_gdf = gpd.GeoDataFrame([initial_row], crs=parks_gdf.crs)\n",
    "buffer_gdf = buffer_gdf.iloc[0:0]  # Clear the initial row, keeping dtypes\n",
    "\n",
    "\n",
    "for park_id, park in parks_gdf.iterrows():\n",
    "    for dist in buffer_distances:\n",
    "        # Create donut-shaped buffers (outer - inner)\n",
    "        outer = park.geometry.buffer(dist)\n",
    "        inner = park.geometry.buffer(max(0, dist-30))  # Handle 0-30m case\n",
    "        ring = outer.difference(inner)\n",
    "        \n",
    "        # Append to GeoDataFrame\n",
    "        buffer_gdf = pd.concat([\n",
    "            buffer_gdf,\n",
    "            gpd.GeoDataFrame({\n",
    "                'Park_id': [int(park['Park_id'])],  # Use actual Park_id for accurate mapping\n",
    "                'distance': [dist],\n",
    "                'geometry': [ring]\n",
    "            }, crs=parks_gdf.crs)\n",
    "        ], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2210 buffer rings\n",
      "GeoDataFrame columns: Index(['Park_id', 'distance', 'geometry'], dtype='object')\n",
      "CRS: EPSG:20438\n",
      "Sample data:\n",
      "   Park_id  distance                                           geometry\n",
      "0        1        30  POLYGON ((678371.261 2735243.793, 678368.638 2...\n",
      "1        1        60  POLYGON ((678387.297 2735218.438, 678382.05 27...\n",
      "2        1        90  POLYGON ((678403.333 2735193.084, 678395.463 2...\n"
     ]
    }
   ],
   "source": [
    "# Verify the output structure\n",
    "print(f\"Generated {len(buffer_gdf)} buffer rings\")\n",
    "print(\"GeoDataFrame columns:\", buffer_gdf.columns)\n",
    "print(\"CRS:\", buffer_gdf.crs)\n",
    "print(\"Sample data:\")\n",
    "print(buffer_gdf.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved buffers to GeoJSON\n"
     ]
    }
   ],
   "source": [
    "# # Save to GeoJSON\n",
    "# ## Commented out as it's needed once\n",
    "# buffer_gdf.to_file('Data/Riyadh_parks_buffers.geojson', driver='GeoJSON')\n",
    "# print(\"Saved buffers to GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LST Calculation\n",
    "\n",
    "Here, the pixel-polygon overlap appraoch is used for mean calculation. Check Section 2.2 of '04_FactorsCalculations.ipynb' for justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buffer columns for LST means and pixel counts in parks_gdf (park interiors) and buffer_gdf (buffer zones)\n",
    "for dist in range(30, 301, 30):\n",
    "    parks_gdf[f'lst_buffer_{dist}m'] = np.nan\n",
    "\n",
    "parks_gdf['lst_park'] = np.nan\n",
    "parks_gdf['park_pixel_count'] = 0\n",
    "buffer_gdf['lst_mean'] = np.nan\n",
    "buffer_gdf['pixel_count'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calculate Mean LST within each park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repaired invalid geometry for park 155.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate LST for park interiors manually with fractional overlap\n",
    "# Followin NDVI manual method for consistency, weighting pixels >50% within park\n",
    "with rasterio.open(lst_2024_path) as lst_src:\n",
    "    # Get raster properties for overlap calculation\n",
    "    pixel_area = abs(lst_src.transform[0] * lst_src.transform[4])  # 30m x 30m = 900 m²\n",
    "    threshold_area = 0.50 * pixel_area  # 50% threshold, 450 m²\n",
    "    raster_nodata = lst_src.nodata if lst_src.nodata is not None else -999\n",
    "\n",
    "    for idx, park in parks_gdf.iterrows():\n",
    "        geom = park.geometry\n",
    "        if not geom.is_valid:\n",
    "            geom = geom.buffer(0)\n",
    "            print(f\"Repaired invalid geometry for park {park['Park_id']}\")\n",
    "\n",
    "        # Rasterize park mask with all_touched\n",
    "        mask = rasterio.features.rasterize(\n",
    "            [(geom, 1)],\n",
    "            out_shape=(lst_src.height, lst_src.width),\n",
    "            transform=lst_src.transform,\n",
    "            fill=0,\n",
    "            default_value=1,\n",
    "            all_touched=True\n",
    "        ) == 1\n",
    "\n",
    "        # Extract and weight LST values\n",
    "        lst_park_data = lst_src.read(1)\n",
    "        y_indices, x_indices = np.where(mask)\n",
    "        total_weighted_sum = 0\n",
    "        total_weighted_area = 0\n",
    "\n",
    "        for y, x in zip(y_indices, x_indices):\n",
    "            pixel_geom = box(\n",
    "                lst_src.transform[2] + x * lst_src.transform[0],  # minx\n",
    "                lst_src.transform[5] + y * lst_src.transform[4],  # miny\n",
    "                lst_src.transform[2] + (x + 1) * lst_src.transform[0],  # maxx\n",
    "                lst_src.transform[5] + (y + 1) * lst_src.transform[4]  # maxy\n",
    "            )\n",
    "            intersection = geom.intersection(pixel_geom)\n",
    "            if not intersection.is_empty:\n",
    "                overlap_area = intersection.area\n",
    "                if overlap_area >= threshold_area:  # Only include >50% overlap\n",
    "                    lst_value = lst_park_data[y, x]\n",
    "                    if lst_value != raster_nodata:\n",
    "                        total_weighted_sum += lst_value * overlap_area\n",
    "                        total_weighted_area += overlap_area\n",
    "                        parks_gdf.at[idx, 'park_pixel_count'] += 1\n",
    "\n",
    "        # Calculate weighted mean for park interior\n",
    "        if total_weighted_area > 0:\n",
    "            parks_gdf.at[idx, 'lst_park'] = total_weighted_sum / total_weighted_area\n",
    "        else:\n",
    "            print(f\"Park {park['Park_id']} has no valid overlapping pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculate Mean LST within each buffer zone for each park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LST for buffer zones manually with fractional overlap\n",
    "# Mirrors NDVI approach, ensuring consistent weighting across buffer distances\n",
    "with rasterio.open(lst_2024_path) as lst_src:\n",
    "    # Reuse raster properties for buffer calculations\n",
    "    pixel_area = abs(lst_src.transform[0] * lst_src.transform[4])  # 30m x 30m = 900 m²\n",
    "    threshold_area = 0.50 * pixel_area  # 50% threshold, 450 m²\n",
    "    raster_nodata = lst_src.nodata if lst_src.nodata is not None else -999\n",
    "\n",
    "    for idx, buffer in buffer_gdf.iterrows():\n",
    "        geom = buffer.geometry\n",
    "        park_id = buffer['Park_id']\n",
    "        dist = buffer['distance']\n",
    "\n",
    "        if not geom.is_valid:\n",
    "            geom = geom.buffer(0)\n",
    "            print(f\"Repaired invalid geometry for buffer of park {park_id} at {dist}m\")\n",
    "\n",
    "        # Rasterize buffer mask with all_touched\n",
    "        mask = rasterio.features.rasterize(\n",
    "            [(geom, 1)],\n",
    "            out_shape=(lst_src.height, lst_src.width),\n",
    "            transform=lst_src.transform,\n",
    "            fill=0,\n",
    "            default_value=1,\n",
    "            all_touched=True\n",
    "        ) == 1\n",
    "\n",
    "        # Extract and weight LST values for buffer\n",
    "        lst_buffer_data = lst_src.read(1)\n",
    "        y_indices, x_indices = np.where(mask)\n",
    "        total_weighted_sum = 0\n",
    "        total_weighted_area = 0\n",
    "\n",
    "        for y, x in zip(y_indices, x_indices):\n",
    "            pixel_geom = box(\n",
    "                lst_src.transform[2] + x * lst_src.transform[0],  # minx\n",
    "                lst_src.transform[5] + y * lst_src.transform[4],  # miny\n",
    "                lst_src.transform[2] + (x + 1) * lst_src.transform[0],  # maxx\n",
    "                lst_src.transform[5] + (y + 1) * lst_src.transform[4]  # maxy\n",
    "            )\n",
    "            intersection = geom.intersection(pixel_geom)\n",
    "            if not intersection.is_empty:\n",
    "                overlap_area = intersection.area\n",
    "                if overlap_area >= threshold_area:  # Only include >50% overlap\n",
    "                    lst_value = lst_buffer_data[y, x]\n",
    "                    if lst_value != raster_nodata:\n",
    "                        total_weighted_sum += lst_value * overlap_area\n",
    "                        total_weighted_area += overlap_area\n",
    "                        buffer_gdf.at[idx, 'pixel_count'] += 1\n",
    "\n",
    "        # Calculate weighted mean for buffer zone\n",
    "        if total_weighted_area > 0:\n",
    "            buffer_gdf.at[idx, 'lst_mean'] = total_weighted_sum / total_weighted_area\n",
    "        else:\n",
    "            print(f\"Buffer for park {park_id} at {dist}m has no valid overlapping pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Appending results to parks_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** lst_park (interior LST) is calculated and added separately in sec 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results include:\n",
      "- 221 parks processed\n",
      "- Cooling effects calculated for buffers 30-300m\n"
     ]
    }
   ],
   "source": [
    "# Adding LST buffer values to parks_gdf\n",
    "for park_id in parks_gdf['Park_id'].unique():\n",
    "    park_buffers = buffer_gdf[buffer_gdf['Park_id'] == park_id]\n",
    "    for _, buffer in park_buffers.iterrows():\n",
    "        dist = buffer['distance']\n",
    "        lst_value = buffer['lst_mean']\n",
    "        if lst_value is not None and not np.isnan(lst_value):\n",
    "            parks_gdf.loc[parks_gdf['Park_id'] == park_id, f'lst_buffer_{dist}m'] = lst_value\n",
    "\n",
    "print(\"Processing complete. Results include:\")\n",
    "print(f\"- {len(parks_gdf)} parks processed\")\n",
    "print(f\"- Cooling effects calculated for buffers 30-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of parks with valid lst_park: 221 (3304 pixels)\n",
      "- Number of buffers with valid lst_mean: 2210 (148420 pixels)\n"
     ]
    }
   ],
   "source": [
    "print(f\"- Number of parks with valid lst_park: {parks_gdf['lst_park'].notna().sum()} ({parks_gdf['park_pixel_count'].sum()} pixels)\")\n",
    "print(f\"- Number of buffers with valid lst_mean: {buffer_gdf['lst_mean'].notna().sum()} ({buffer_gdf['pixel_count'].sum()} pixels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCI - PCE ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cooling effects (ΔT between buffers and park interior)\n",
    "for dist in range(30, 301, 30):\n",
    "    parks_gdf[f'cooling_diff_{dist}m'] = (\n",
    "        parks_gdf[f'lst_buffer_{dist}m'] - parks_gdf['lst_park']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out export, only needed once:\n",
    "parks_gdf.to_file('Data/Riyadh_parks_with_LST_Overlap.geojson', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf['max_cooling'] = parks_gdf[[f'cooling_diff_{d}m' for d in range(30, 301, 30)]].min(axis=1)\n",
    "\n",
    "parks_gdf['cooling_radius'] = (\n",
    "    parks_gdf[[f'cooling_diff_{d}m' for d in range(30, 301, 30)]]\n",
    "    .idxmin(axis=1)\n",
    "    .str.extract(r'(\\d+)')\n",
    "    .astype(int)\n",
    ")\n",
    "# Verify results\n",
    "print(parks_gdf[['park_id', 'lst_park', 'max_cooling', 'cooling_radius']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "avg_max_cooling = parks_gdf['max_cooling'].mean()\n",
    "cooling_park_count = len(parks_gdf[parks_gdf['max_cooling'] < 0])\n",
    "heat_island_count = len(parks_gdf[parks_gdf['max_cooling'] > 0])\n",
    "avg_radius = parks_gdf['cooling_radius'].mean()\n",
    "\n",
    "print(f\"\"\"\n",
    "All praks summary:\n",
    "1. Average maximum cooling effect: {avg_max_cooling:.1f}°C\n",
    "2. {cooling_park_count} parks ({cooling_park_count/len(parks_gdf):.0%}) provide cooling\n",
    "3. {heat_island_count} parks ({heat_island_count/len(parks_gdf):.0%}) act as heat islands  \n",
    "4. Average cooling radius: {avg_radius:.0f}m\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for cooling differences\n",
    "cooling_diff_stats = pd.DataFrame({\n",
    "    'min': [parks_gdf[f'cooling_diff_{d}m'].min() for d in range(30, 301, 30)],\n",
    "    'max': [parks_gdf[f'cooling_diff_{d}m'].max() for d in range(30, 301, 30)],\n",
    "    'mean': [parks_gdf[f'cooling_diff_{d}m'].mean() for d in range(30, 301, 30)],\n",
    "    'std': [parks_gdf[f'cooling_diff_{d}m'].std() for d in range(30, 301, 30)]\n",
    "}, index=[f'{d}m' for d in range(30, 301, 30)])\n",
    "print(\"\\nSummary Statistics for Cooling Differences:\")\n",
    "print(cooling_diff_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show varying cooling/heating impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 cooling parks\n",
    "top_coolers = parks_gdf.nsmallest(5, 'max_cooling')[['park_id', 'impervious_prop','max_cooling', 'cooling_radius', 'area_m2', 'ndvi_mean']]\n",
    "\n",
    "print(\"\\nTop Cooling Parks:\")\n",
    "print(top_coolers.to_markdown(index=False))\n",
    "\n",
    "# Calculate their common traits\n",
    "avg_top_ndvi = top_coolers['ndvi_mean'].mean()\n",
    "avg_top_area = top_coolers['area_m2'].mean()\n",
    "avg_heat_paved = top_coolers['impervious_prop'].mean()\n",
    "\n",
    "print(f\"\\nCommon Traits:\\n- Average NDVI: {avg_top_ndvi:.2f}\\n- Average paved surface: {avg_heat_paved:.0%}\\n- Average Area: {avg_top_area:,.0f} m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze worst heat islands\n",
    "heat_islands = parks_gdf.nlargest(5, 'max_cooling')[['park_id','ndvi_mean', 'max_cooling', 'impervious_prop', 'area_m2']]\n",
    "\n",
    "print(\"\\nWorst Heat Islands:\")\n",
    "print(heat_islands.to_markdown(index=False))\n",
    "\n",
    "# Calculate their characteristics\n",
    "avg_heat_paved = heat_islands['impervious_prop'].mean()\n",
    "avg_heat_size = heat_islands['area_m2'].mean()\n",
    "avg_heat_ndvi = heat_islands['ndvi_mean'].mean()\n",
    "\n",
    "print(f\"\\nCommon Issues:\\n- Average NDVI: {avg_top_ndvi:.2f}\\n- Average paved surface: {avg_heat_paved:.0%}\\n- Average size: {avg_heat_size:,.0f} m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean cooling at each distance\n",
    "distances = [0] + list(range(30, 301, 30))\n",
    "mean_cooling = [0] + [parks_gdf[f'cooling_diff_{d}m'].mean() for d in range(30, 301, 30)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(distances, mean_cooling, color='#2e8b57', linewidth=2, marker='o')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.fill_between(distances, mean_cooling, 0, where=np.array(mean_cooling)<0, \n",
    "                 color='#9fd5b3', alpha=0.3)\n",
    "plt.title(\"Mean Temperature Differential by Distance from Park Edge\", pad=20)\n",
    "plt.xlabel(\"Distance (m)\")\n",
    "plt.ylabel(\"Δ Temperature (°C)\")\n",
    "plt.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vegetation-rich vs. paved parks\n",
    "plt.figure(figsize=(10,5))\n",
    "for group, color in [('High Veg', 'green'), ('Low Veg', 'brown')]:\n",
    "    subset = parks_gdf[parks_gdf['vegetation_prop'] > 0.5] if group == 'High Veg' else parks_gdf[parks_gdf['vegetation_prop'] <= 0.5]\n",
    "    means = [subset[f'cooling_diff_{d}m'].mean() for d in range(30,301,30)]\n",
    "    plt.plot(distances[1:], means, label=group, color=color, marker='o')\n",
    "\n",
    "plt.axhline(0, color='gray', ls='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LST (not ΔT) to see absolute temperatures\n",
    "plt.figure(figsize=(10,6))\n",
    "for label, color in [('High Veg', 'green'), ('Low Veg', 'gray')]:\n",
    "    subset = parks_gdf[parks_gdf['vegetation_prop'] > 0.5] if label == 'High Veg' else parks_gdf[parks_gdf['vegetation_prop'] <= 0.5]\n",
    "    plt.plot(distances[1:], \n",
    "             [subset[f'lst_buffer_{d}m'].mean() for d in range(30,301,30)], \n",
    "             label=label, color=color, marker='o')\n",
    "\n",
    "plt.plot([0, 300], [subset['lst_park'].mean()]*2, '--', color='black', label='Park Interior')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Absolute LST (°C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate parks (Identified from QGIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_212 = parks_gdf[parks_gdf['park_id'] == 212].iloc[0]\n",
    "print(f\"\"\"\n",
    "Park 212 Profile:\n",
    "- Name: {park_212['FEATURE_ANAME']}\n",
    "- Area: {park_212['area_m2']:,.0f} m²\n",
    "- NDVI: {park_212['ndvi_mean']:.2f}\n",
    "- Vegetation Cover: {park_212['vegetation_prop']:.0%}\n",
    "- Impervious: {park_212['impervious_prop']:.0%}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "distances = range(30, 301, 30)\n",
    "park_lst = park_212['lst_park']\n",
    "buffer_lst = [park_212[f'lst_buffer_{d}m'] for d in distances]\n",
    "cooling = [park_212[f'cooling_diff_{d}m'] for d in distances]\n",
    "\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "# Absolute temperatures\n",
    "ax1.plot([0] + list(distances), [park_lst] + buffer_lst, \n",
    "         marker='o', color='red')\n",
    "ax1.set_title(f\"Park 212: LST Profile\\n(Park Interior = {park_lst:.1f}°C)\")\n",
    "ax1.set_xlabel(\"Distance from Edge (m)\")\n",
    "ax1.set_ylabel(\"Land Surface Temperature (°C)\")\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Cooling effect\n",
    "ax2.plot(distances, cooling, marker='o', color='blue')\n",
    "ax2.axhline(0, color='gray', ls='--')\n",
    "ax2.set_title(\"Cooling Effect (ΔT = Buffer - Park)\")\n",
    "ax2.set_xlabel(\"Distance from Edge (m)\")\n",
    "ax2.set_ylabel(\"Δ Temperature (°C)\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temperature profile\n",
    "distances = range(0, 301, 30)\n",
    "temps = [park_212['lst_park']] + [park_212[f'lst_buffer_{d}m'] for d in distances[1:]]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(distances, temps, marker='o', color='darkred')\n",
    "plt.title(f\"Park 212: Thermal Profile\\n(Area: {park_212['area_m2']/10000:.1f} ha, NDVI: {park_212['ndvi_mean']:.2f})\")\n",
    "plt.xlabel(\"Distance from Edge (m)\")\n",
    "plt.ylabel(\"LST (°C)\")\n",
    "plt.grid()\n",
    "plt.annotate(f\"Max ΔT: +{park_212['cooling_diff_210m']:.1f}°C at 210m\", \n",
    "             xy=(210, 51.8), xytext=(100, 53),\n",
    "             arrowprops=dict(facecolor='black'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to typical parks\n",
    "median_park = parks_gdf[parks_gdf['vegetation_prop'] > 0.6]['lst_park'].median()\n",
    "print(f\"Median LST for vegetated parks: {median_park:.1f}°C vs Park 212: {park_212['lst_park']:.1f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For COnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(parks_gdf['max_cooling'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.axvline(parks_gdf['max_cooling'].mean(), color='red', linestyle='--', label=f'Mean: {parks_gdf[\"max_cooling\"].mean():.2f}°C')\n",
    "plt.title('Distribution of Maximum Cooling Effect')\n",
    "plt.xlabel('Maximum Cooling Effect (°C)')\n",
    "plt.ylabel('Number of Parks')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf['max_cooling'] = parks_gdf[[f'cooling_diff_{d}m' for d in range(30, 301, 30)]].min(axis=1)\n",
    "\n",
    "# Fix cooling_radius calculation\n",
    "parks_gdf['cooling_radius'] = (\n",
    "    parks_gdf[[f'cooling_diff_{d}m' for d in range(30, 301, 30)]]\n",
    "    .idxmin(axis=1)\n",
    "    .str.extract(r'(\\d+)')  # Extract numeric part\n",
    "    .iloc[:, 0]  # Ensure single column extraction\n",
    "    .str.strip('m')  # Remove any stray 'm' characters\n",
    "    .astype(float)  # Convert to float first to handle NaN\n",
    "    .fillna(0)  # Replace NaN with 0\n",
    "    .astype(int)  # Convert to int\n",
    ")\n",
    "\n",
    "# Verify results, focusing on Park 212\n",
    "park_212 = parks_gdf[parks_gdf['park_id'] == 212].iloc[0]\n",
    "print(f\"Park 212: lst_park = {park_212['lst_park']:.1f}°C, max_cooling = {park_212['max_cooling']:.2f}°C, cooling_radius = {park_212['cooling_radius']}m\")\n",
    "print(parks_gdf[['park_id', 'lst_park', 'max_cooling', 'cooling_radius']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Scatter plot for max_cooling vs. NDVI and area\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "# NDVI vs. max_cooling\n",
    "slope_ndvi, intercept_ndvi, r_value_ndvi, p_value_ndvi, _ = linregress(parks_gdf['ndvi_mean'], parks_gdf['max_cooling'])\n",
    "line_ndvi = slope_ndvi * parks_gdf['ndvi_mean'] + intercept_ndvi\n",
    "ax1.scatter(parks_gdf['ndvi_mean'], parks_gdf['max_cooling'], color='#ADD8E6', alpha=0.5, s=50)\n",
    "ax1.plot(parks_gdf['ndvi_mean'], line_ndvi, color='#1E90FF', label=f'R² = {r_value_ndvi**2:.2f}')\n",
    "park_212_ndvi = parks_gdf[parks_gdf['park_id'] == 212]['ndvi_mean'].iloc[0]\n",
    "park_212_cooling = parks_gdf[parks_gdf['park_id'] == 212]['max_cooling'].iloc[0]\n",
    "ax1.scatter(park_212_ndvi, park_212_cooling, color='#4682B4', s=100, label='Park 212')\n",
    "ax1.set_title('NDVI vs. Maximum Cooling Effect', fontsize=12)\n",
    "ax1.set_xlabel('NDVI', fontsize=10)\n",
    "ax1.set_ylabel('Maximum Cooling Effect (°C)', fontsize=10)\n",
    "ax1.grid(linestyle='--', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# Area vs. max_cooling\n",
    "slope_area, intercept_area, r_value_area, p_value_area, _ = linregress(parks_gdf['area_m2'], parks_gdf['max_cooling'])\n",
    "line_area = slope_area * parks_gdf['area_m2'] + intercept_area\n",
    "ax2.scatter(parks_gdf['area_m2']/10000, parks_gdf['max_cooling'], color='#ADD8E6', alpha=0.5, s=50)\n",
    "ax2.plot(parks_gdf['area_m2']/10000, line_area, color='#1E90FF', label=f'R² = {r_value_area**2:.2f}')\n",
    "park_212_area = parks_gdf[parks_gdf['park_id'] == 212]['area_m2'].iloc[0]/10000\n",
    "ax2.scatter(park_212_area, park_212_cooling, color='#4682B4', s=100, label='Park 212')\n",
    "ax2.set_title('Park Area vs. Maximum Cooling Effect', fontsize=12)\n",
    "ax2.set_xlabel('Area (ha)', fontsize=10)\n",
    "ax2.grid(linestyle='--', alpha=0.7)\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Separate cooling and heat island parks\n",
    "cooling_parks = parks_gdf[parks_gdf['max_cooling'] < 0]\n",
    "heat_islands = parks_gdf[parks_gdf['max_cooling'] > 0]\n",
    "\n",
    "# Calculate average values for key factors\n",
    "factors = ['ndvi_mean', 'area_m2', 'impervious_prop', 'bare_sparse_prop', 'LSI']\n",
    "cooling_avg = cooling_parks[factors].mean()\n",
    "heat_avg = heat_islands[factors].mean()\n",
    "\n",
    "# Perform t-test for NDVI to check significance\n",
    "t_stat, p_value = ttest_ind(cooling_parks['ndvi_mean'], heat_islands['ndvi_mean'])\n",
    "sig_note = f\"p-value NDVI: {p_value:.3f}\" if p_value < 0.05 else f\"p-value NDVI: {p_value:.3f} (ns)\"\n",
    "\n",
    "# Create grouped bar chart with dual y-axes\n",
    "x = np.arange(len(factors))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax1.bar(x - width/2, [cooling_avg[f] for f in factors[:-1]] + [cooling_avg['area_m2']], width, \n",
    "                label='Cooling Parks (44%)', color='#ADD8E6')\n",
    "bars2 = ax1.bar(x + width/2, [heat_avg[f] for f in factors[:-1]] + [heat_avg['area_m2']], width, \n",
    "                label='Heat Islands (56%)', color='#4682B4')\n",
    "\n",
    "# Secondary y-axis for area_m2\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim(ax1.get_ylim()[0] * 10000, ax1.get_ylim()[1] * 10000)  # Scale to m²\n",
    "ax2.set_ylabel('Area (m²)', fontsize=10, color='#1E90FF')\n",
    "ax1.set_ylabel('Average Value (Unitless)', fontsize=10)\n",
    "\n",
    "# Customize plot\n",
    "ax1.set_title('Contributing Factors: Cooling vs. Heat Island Parks', fontsize=12)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f.replace('_m2', '') for f in factors], rotation=45, ha='right')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "ax1.text(0.5, -0.1, sig_note, transform=ax1.transAxes, ha='center', color='#1E90FF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract Park 163 data\n",
    "park_163 = parks_gdf.iloc[162]  # Index 162 for park_id 163\n",
    "distances = range(0, 301, 30)\n",
    "temps = [park_163['lst_park']] + [park_163[f'lst_buffer_{d}m'] for d in range(30, 301, 30)]\n",
    "\n",
    "# Plot thermal profile\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(distances, temps, marker='o', color='#4682B4', label='LST')\n",
    "plt.title(f\"Park 163: Thermal Profile\\n(Estimated Area: ~20,000 m², NDVI: ~0.3)\", fontsize=12)\n",
    "plt.xlabel(\"Distance from Edge (m)\", fontsize=10)\n",
    "plt.ylabel(\"LST (°C)\", fontsize=10)\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "max_dt = max(park_163[[f'cooling_diff_{d}m' for d in range(30, 301, 30)]])\n",
    "max_dist = park_163[[f'cooling_diff_{d}m' for d in range(30, 301, 30)]].idxmax().split('_')[-1]\n",
    "plt.annotate(f\"Max ΔT: +{max_dt:.2f}°C at {max_dist}m\", \n",
    "             xy=(int(max_dist), park_163[f'lst_buffer_{max_dist}m']), xytext=(100, 52),\n",
    "             arrowprops=dict(facecolor='#1E90FF'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_163 = parks_gdf[parks_gdf['park_id'] == 163].iloc[0]\n",
    "print(f\"\"\"\n",
    "Park 163 Profile:\n",
    "- Name: {park_163['FEATURE_ANAME']}\n",
    "- Area: {park_163['area_m2']:,.0f} m²\n",
    "- NDVI: {park_163['ndvi_mean']:.2f}\n",
    "- LST: {park_163['lst_park']:.2f}\n",
    "- Vegetation Cover: {park_163['vegetation_prop']:.0%}\n",
    "- Impervious: {park_163['impervious_prop']:.0%}\n",
    "- Max Coolin:{park_163['max_cooling']}\n",
    "- Mx Radious: {park_163['cooling_radius']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_gdf.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
